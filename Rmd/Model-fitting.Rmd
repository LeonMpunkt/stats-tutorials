---
title: "Model fitting"
author: "LT"
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
    df_print: "paged"
    keep_md: TRUE
---

```{r}
library(ggplot2)
library(GGally) # for a scatterplot matrix
```

# User-defined functions

We have seen functions at work in R already. A function takes one or more inputs in parentheses, and returns to us some output.

For example, the natural logarithm function.

```{r}
log(9000)
```

We may define our own functions in R. To define a function, we use `function()`. In the parentheses we give names to the expected input(s) of the function. We then open the curly braces `{}`. Inside the curly braces we write some R commands that carry out the operations we want our function to perform. In carrying out these operations, we refer to the inputs by the names we gave them. When our operations are complete, we 'return' the result. Whatever we place inside the parentheses of `return()` will be the output of the function. We can store a function using `=`, just as we store anything else.

We have seen a brief example of this already in a previous class, when we defined a function to calculate the square root of the mean.

As a further example, here is a function for calculating the estimated standard error of a mean, given the standard deviation `sd` and sample size `n` as inputs.

```{r}
se = function(sd, n){
  return(sd / sqrt(n))
}
```

Once we have stored a function, we may use it just like any other function in R. We refer to it by the name under which we stored it, and place the inputs in the parentheses, in the same order that we used when defining the function.

So to use the function above to calculate the standard error from an observed standard deviation of 2.5 and a sample size of 20:

```{r}
se(2.5, 20)
```

One common use of functions is to define a model that relates some predictor variable to an outcome variable. In this case, the inputs to the function are the values of the predictor variable, plus some parameters of the model, and the output of the function is the predicted value of the outcome variable.

For example, we can define a simple linear regression model. The function takes the value of the predictor variable *x* as its input, as well as the two parameters of the regression line, its intercept and its slope. For the sake of clarity, it is often a good idea to give the parameters of the model meaningful names.

```{r}
linear_mod = function(x, intercept, slope){
  return(intercept + slope*x)
}
```

When using a function as a model, we can make our R commands a little clearer by naming the parameter inputs, using `=`. This is optional, but it makes it easier to see what parameters are going into the model, and also makes it more obvious to us if we have entered them incorrectly.

```{r}
linear_mod(70, intercept=2, slope=0.01)
```

Naming the inputs also ensures that the correct values are allocated to the correct parameters, even if we have entered them in a different order from the one we defined when creating the function.

```{r}
linear_mod(70, slope=0.01, intercept=2)
```

# Model fitting

Once we have defined a model as a function relating predictor(s) to outcome, we can fit this model to a given set of data. Fitting the model to the data involves finding values for the parameters that produce predicted values of the outcome that in some way 'best fit' the actual observed values.

## Least squares

What defines a 'best fit'? There are several different ways of defining it. One is to consider the differences between the values that the model predicted and the observed values, and then to summarize these differences in some way. A common choice is to square the differences and calculate their mean (and then optionally take the square root of this number to bring the value back onto the original scale of measurement of the outcome). This measure of the fit of a model is termed the *R*oot *M*ean *S*quared *E*rror (RMSE), and we encountered it before when we used it as a measure of predictive accuracy in cross-validation.

The best fitting values of the model's parameters are those values that make this (or some other) measure of the error the smallest, meaning that this model's predictions are as close to the real observations as possible.

Since we have started working with functions, and since we will be calculating the RMSE a few times later on, let's define a function for calculating it. The function's inputs will be the predicted and observed values.

```{r}
rmse = function(predicted, observed){
  errors = predicted - observed
  sqerrors = errors^2
  return(sqrt(mean(sqerrors)))
}
```

You might be wondering why we bother to square the errors. One reason is that it ensures that all errors are positive, so that negative and positive errors do not cancel out. But if this were the only reason, we could just achieve the same thing by using the absolute values of the errors.

Using squared rather than absolute errors has another advantage. Unlike a function using absolute values, the RMSE penalizes large errors disproportionately more than it does small errors. To see why this may sometimes be desirable, consider the data below.

(We create some made-up data within R for the purposes of demonstration.)

No straight line provides a really great fit to these data, but intuitively the solid line that passes through the center of the group of observations is the best compromise. This line is indeed the one that minimizes the squared differences between its predictions for the outcome and the observed values of the outcome. It also minimizes the absolute differences.

```{r}
d = data.frame(Outcome=c(1,2,2,3), Predictor=c(1,1,3,3))

fig1 = ggplot(d, aes(y=Outcome, x=Predictor)) +
  geom_point() +
  geom_abline(intercept=1, slope=0.5) +
  coord_equal() +
  lims(y=c(0,4), x=c(0,4))

print(fig1 +
        geom_rect(xmin=0.5, xmax=1, ymin=1.5, ymax=2) +
        geom_rect(xmin=2.5, xmax=3, ymin=2.5, ymax=3) +
        geom_rect(xmin=0.5, xmax=1, ymin=1, ymax=1.5) +
        geom_rect(xmin=2.5, xmax=3, ymin=2, ymax=2.5))
```

Now consider the dashed line below. This line does not accord with our intuition that the values of the outcome are on average increasing with the values of the predictor. Nonetheless, it still minimizes the absolute differences. However, it does not minimize the squared differences, because it produces some differences that are larger than any of those that the solid line produces.

```{r}
fig1 = fig1 + geom_abline(intercept=2, slope=0, lty='dashed')

print(fig1 +
        geom_rect(xmin=0, xmax=1, ymin=1, ymax=2) +
        geom_rect(xmin=2, xmax=3, ymin=2, ymax=3))
```

There can be more than one line that minimizes the absolute differences, such as those shown below (and many more in between). But there is only ever one line that minimizes the squared differences.

```{r}
print(fig1 +
        geom_abline(intercept=0.5, slope=0.5, lty='dashed') +
        geom_abline(intercept=1.5, slope=0.5, lty='dashed'))
```

So using the squared differences as our measure of fit will give us a unique best fitting line, and also one that intuitively 'goes through the middle' of the overall trend in the data, at least to the extent that the form of the model permits.

The method of 'least squares' consists in finding the values for a linear model that minimize the squared differences between predicted and observed values of the outcome.

Let's now see an example using the birth weights data. We will use the data to estimate a model of birth weight as a linear function of mother's weight. We do this first of all just using R's own `lm()` function, which applies least squares.

```{r}
bw = read.csv('birth_weights.csv')
model = lm(Birth_weight ~ Weight, bw)
print(model)
```

We can now calculate the RMSE for the estimated model, using the `rmse()` function that we defined ourselves.

For the linear model of birth weight that we just estimated, we can get the predicted values with the `predict()` function, and the observed values are just the observed birth weights in the data.

```{r}
rmse_fit = rmse(predict(model), bw$Birth_weight)
print(rmse_fit)
```

If `lm()` has done its work and has found the parameter values for the linear model that best fit the data, then this RMSE should be the smallest RMSE possible for this model family.

To see what sort of values of RMSE are possible for a linear model of these data, we can turn to the `linear_mod()` function that we defined for ourselves above. We can try inputting many different values of the intercept and slope parameters, along with the observed mothers' weights, and see what predicted birth weights these combinations of values give us. We can then input these predictions into our `rmse()` function along with the observed birth weights, to get an RMSE value for each combination of intercept and slope.

We begin by defining a range of values that we want to try out for each parameter. We can get a range of values using the R function `seq()` (short for *seq*uence). The inputs say where the sequence of values should begin, where it should end, and how many values to create in between (`length.out`).

```{r}
n_values = 100

intercepts = seq(2, 3, length.out=n_values)

slopes = seq(0, 0.02, length.out=n_values)
```

We would like to try out every combination of these values. The R function `expand.grid()` generates a data frame that contains all the possible combinations of two or more variables with multiple values.

```{r}
models = expand.grid(intercept=intercepts, slope=slopes)
head(models)
```

Now we can go through each of these combinations of intercept and slope, and ask how well the resulting regression model fits the observed birth weights data. For this, we use both of our functions from above: `linear_mod()` to generate the model's predictions, and `rmse()` to compare these to the observed birth weights.

We first add a new column to the data frame of parameter combinations, then fill it in a loop.

```{r}
models$RMSE = 0

for(m in 1:nrow(models)){
  predicted = linear_mod(bw$Weight, intercept=models$intercept[m], slope=models$slope[m])
  models$RMSE[m] = rmse(predicted, bw$Birth_weight)
}

head(models)
```

We have now tried out many different combinations of values for intercept and slope. We can visualize how well these fit the data by mapping the range of slopes and intercepts to the *x* and *y* axes, and mapping their RMSE values to a color scale.

The `geom_raster()` function from ggplot can be used to fill the area of a plot with colored tiles. For a bit of extra clarity, we can also add to the plot a custom color scale, defining which colors we want to use for the low and high ends of the scale. Color gradients can be a little difficult to perceive in detail, so we also add contour lines to show where RMSE values are getting lower. Countour lines use the `z` aesthetic mapping to determine the countours. Finally, we add dashed lines representing the best fit values, which we get from the model that we fit with `lm()` above.

```{r}
fig2 = ggplot(models, aes(x=slope, y=intercept, z=RMSE, fill=RMSE)) +
  geom_raster() +
  geom_contour(binwidth=0.01, color='black') +
  scale_fill_gradient(low='red', high='yellow') +
  geom_hline(yintercept=coefficients(model)[1], lty='dashed') +
  geom_vline(xintercept=coefficients(model)[2], lty='dashed')

print(fig2)
```

We see that there are various combinations of intercept and slope that fit the data almost as well as the best fit values, and the fit gets worse the further away from these values we go.

We can also check that among the models we tried out, the one that best fits the data has an RMSE that is still not quite as low as that of the best possible model for these data as fit by `lm()`.

```{r}
min(models$RMSE)

min(models$RMSE) < rmse_fit
```

In our process of searching through lot of possible models, we get quite close to the best fitting values of intercept and slope.

(We can pick out the row of our data frame where RMSE is at its minimum, and compare the slope and intercept values here to those obtained by `lm()`.)

```{r}
print(models[models$RMSE == min(models$RMSE),])

print(model)
```

## Search

How did `lm()` find the best fitting values that our search just missed? It turns out that for linear models there is just a proven formula for calculating the best fitting values of the model parameters. We can just plug the data and the model form into this formula and get a guaranteed correct answer.

This is not always the case. If the function that we wish to fit to the data is not a linear model, then we don't necessarily have a guaranteed formula for finding the best fitting parameter values.

Can we do better than just searching among the possible parameter values for the best fit we can find? Essentially, no. We have to just search. But we can search a bit more systematically than just trying lots and lots of combinations. Instead, we can start by trying just a few combinations that are fairly close to one another, see which is best, and then 'move' in the direction of the best one, so that next we try out combinations that are close to this one, and so on. In so doing, we move through the space of possible parameter values, always moving in the direction of better and better fits to the data, until we find that fits no longer get any better.

In the simple case where we only need to fit two parameters to the data, we can think of the space of possible parameters values as being a 3-dimensional landscape. Two of the dimensions are the two model parameters, and the third dimension is the fit to the data (RMSE or some other chosen measure), such that 'higher ground' indicates a worse fit, and 'lower ground' indicates a better fit. As we search through this landscape, we are looking for the lowest ground, so we move wherever the landscape leads downwards.

There are a few important subtleties to this procedure, and there are many, many variations on it, but this method is essentially how a lot of model fitting algorithms work.

Before we turn to applying this principle to a non-linear model, let's apply it to the linear regression model we defined above. This is of course unnecessary, because a linear model has a proven formula for its solution, but seeing the search in action on a very simple example will give us a better understanding of how it works before we apply it to a more complex case.

The R function `nls()` (which stands for *n*onlinear *l*east *s*quares) applies an algorithm like this to estimate the parameters of a nonlinear model from a set of data. Using `nls()` works a lot like `lm()`. We input a formula describing the model. This formula includes on the predictor side the function that we have created to represent our model. We put the predictor variable in the function input, along with the names of the model parameters, but without assigning them any values. We also input the data.

One important difference from `lm()` is that we need to tell the algorithm where to begin its search. For this, we need to input a list of starting values for the parameters. If we have plotted our data and have explored the behavior of our function, then we may have an approximate idea of where to search. If not, we can just take a guess or pick some default value. Since the algorithm will move towards better fitting values, it often doesn't matter if we pick an arbitrary starting point. To illustrate this here, we will pick very bad initial values for the intercept and slope.

As noted above, there are lots of different specific algorithms for searching the space of possible parameter values. `nls()` allows us to specify a particular algorithm. To illustrate this, we ask here for the `'port'` algorithm (the precise details of which are a bit much to go into here).

Finally, the `trace=TRUE` option can track for us what values of the model's parameters the search procedure goes through before finding the final fitted values.

```{r}
starting_values = list(intercept=90, slope=90)

fit = nls(Birth_weight ~ linear_mod(Weight, intercept, slope), bw,
          starting_values,
          algorithm='port',
          trace=TRUE)
```

We can see from the `trace` output that the algorithm initially had to search around a bit. Each row is one step in the search, and the last two columns give the values of the intercept and slope at that step. The algorithm began by searching in a direction that was only approximately right. Because this was nonetheless a direction in which the fit quickly got a lot better, the algorithm 'sped up' its search by making larger steps. This led to an 'overshoot' for the intercept, going into negative values, but then the fact that the fit began getting worse in this direction steered the algorithm towards the region of the best fitting values. It then searched more finely for a couple more steps.

We can plot its path through the space of parameter values. Since the process of saving the search trace into a data frame is a bit tedious, I have pre-prepared this step so that we can load the values from a file.

(The 'RSS' column is the *R*esidual *S*um of *S*quares, a measure of model fit equivalent to the RMSE, just without the Root and Mean part.)

```{r}
trace = read.csv('model_fitting.csv')

trace
```

To give the plot an informative background we first generate a space of RMSE values, as we did for the plot above.

```{r}
models = expand.grid(intercept=seq(-3000, 100, length.out=n_values),
                     slope=seq(0, 100, length.out=n_values))

models$RMSE = 0
for(m in 1:nrow(models)){
  predicted = linear_mod(bw$Weight, intercept=models$intercept[m], slope=models$slope[m])
  models$RMSE[m] = rmse(predicted, bw$Birth_weight)
}

fig3 = ggplot(models, aes(x=slope, y=intercept)) +
  geom_raster(aes(fill=RMSE)) +
  geom_contour(aes(z=RMSE), binwidth=100, color='black') +
  geom_line(data=trace, lwd=1) +
  geom_point(data=trace, shape='circle filled', fill='grey', size=3) +
  scale_fill_gradient(low='red', high='yellow')

print(fig3)
```

In the printed output for the model we see the final fitted values of the intercept and slope, as we do with `lm()`.

```{r}
print(fit)
```

Are these exactly the same as the absolute best values obtained from the formula that `lm()` uses?

```{r}
coefficients(fit)
coefficients(model)

coefficients(fit) == coefficients(model)
```

They are not exactly the same, though the difference is so small that it does not show in the printed output. This is because the search algorithm only ever gets steadily closer to the best-fitting values. At some point it needs to decide when to stop. It does this by checking how much the fit improves each time it searches further, and it stops when the improvement is below some very small value (often termed the 'tolerance'). If we really need extra fine precision in our estimates of the best-fitting parameters, we can adjust the tolerance, but this is not usually necessary.

# A non-linear model

Let's look at an example of fitting a non-linear function to some data. We will use data from a series of experiments on categorization. Participants in the experiments were shown images of crabs and lobsters. These images were morphed so that some of them were mostly lobster-like, some were mostly crab-like, and some were intermediate in shape. The participants then had to rate how lobster-like they thought each image was.

There was a bit more going on in the experiments, but this is the aspect that we will focus on. You can read more about the data [here](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01905/full).

The 'Image' variable tracks what image was being shown, on a standardized scale from 0 to 1, where 0 is maximally crab-like, and 1 is maximally lobster-like. The 'Rating' variable tracks the participants' ratings, on the same scale.

```{r}
crabsters = read.csv('crabsters.csv')
head(crabsters)
```

We can see that each participant rated each image multiple times, over many trials of the experiment.

```{r}
table(crabsters$ID, crabsters$Image)
```

To get an idea of each participant's general reaction to the images, averaging out some trial-to-trial variability, we will calculate the mean ratings per participant per image.

```{r}
mean_ratings = aggregate(Rating ~ Image*ID, crabsters, mean)
mean_ratings
```

Let's now plot these ratings. For a bit of extra plotting practice, we add some new features to the plot:

* overall means as a line, using `stat_summary()`, with `mean` as the summary function for the *y* values
* fixed limits for the *x* axis, using `lims()`
* custom labels for the *y* axis showing the meaning of the rating values

```{r}
fig_crabsters = ggplot(mean_ratings, aes(y=Rating, x=Image)) +
  geom_point(shape='circle filled', fill='grey', position=position_jitter(height=0, width=0.02)) +
  stat_summary(geom='line', fun.y=mean) +
  lims(x=c(0,1)) +
  scale_y_continuous(breaks=c(0,0.5,1), labels=c('0: Crab',0.5,'1: Lobster')) +
  labs(caption='Data: Reindl et al. (2018)')

print(fig_crabsters)
```

Two features of the data are important for us here. First, we appear to have an approximately S-shaped function relating image type to rating. So the participants do not change their opinion of the image by some constant amount as the image changes. Second, the participants do not seem to want to use the full range of the rating scale all the way up to full lobster. (This makes some sense when you look at the 'lobster' images used in the experiments. They don't look so lobster-like.)

One of the secondary objectives of this line of research was to quantify how much more 'suddenly' different kinds of people (novices and experts) change their minds about the categorization of an object when its shape changes. In terms of the trend we see on the plot above, a more 'sudden' switch from one category to another at a certain crucial point in the range of images would be apparent as a steeper, more 'S-like' curve.

We can check how the different participants behaved by faceting our plot by the variable that tracks participant ID.

```{r, fig.width=12, fig.height=9}
fig_crabsters_p = fig_crabsters + facet_wrap(~ID)

print(fig_crabsters_p)
```

To quantify the steepness of the individual participant curves, we can create a model that represents this S-shaped curve, with one of its parameters controlling the steepness of the curve, and then estimate the parameters of the model for each participant. Additionally, such a model will need to take into account the fact that ratings often 'level-off' before reaching the top of the lobsterness scale.

Here is such a model, adapted from an earlier study by the same authors (which you can read about [here](https://www.sciencedirect.com/science/article/pii/S0044523115000224)). As with the simpler linear model example above, we have expressed the model as an R function that takes the predictor value (here the image type) as its first input, followed by the values of the model's parameters, and outputs the predicted value of the outcome (here the rating).

We have given meaningful names to the parameters, that express what aspect of the relationship between predictor and outcome they control:

* asymptote: how far up the rating scale the ratings go
* location: where on the image scale the curve is located
* steepness: how 'steeply' the ratings change

```{r}
f = function(x, asymptote, location ,steepness){
  return(asymptote*(1-2^-((1-x)/(1-location))^-steepness))
}
```

First we should check how the function behaves. We can generate a sequence of *x* values, use our function to calculate the *y* values for certain combinations of parameters, then plot the results.

To keep the plot from getting too cluttered, we try just a few values for each parameter.

(Putting together the data frame of predicted values for each combination of parameter values looks a bit complicated, but uses techniques we have already seen above. We first generate the possible combinations of parameter values, using `expand.grid()` as we did above, but including also the possible *x* values among the combinations, then we input the columns of the resulting data frame into our function to generate the predicted *y* values.)

```{r}
f_data = expand.grid(x=seq(0,1,length.out=100),
                     asymptote=c(0.5,0.8,1),
                     location=c(0.2,0.5,0.8),
                     steepness=c(2,10))

f_data$y = f(f_data$x,
             asymptote=f_data$asymptote,
             location=f_data$location,
             steepness=f_data$steepness)

f_data$steepness = as.factor(f_data$steepness) # (only because we want steepness as discrete line types)

fig_fun = ggplot(f_data, aes(x=x, y=y, lty=steepness)) +
  geom_line() +
  facet_grid(asymptote ~ location, as.table=FALSE, labeller=label_both)

print(fig_fun)
```

As a first step, let's fit the model to the overall mean ratings instead of separately for each participant.

Our model is not linear, so it doesn't have a nice proven formula for estimation like the one that `lm()` uses to find the least squares solution for a linear regression. But there is still a solution somewhere among the possible combinations of parameter values. To get a first idea of where it might be, we can do as we did above and try out some values, checking the fit of each one to our data.

Because we now have three parameters to estimate, we will reduce the number of values we try out in order to keep things manageable.

```{r}
n_values = 16

models = expand.grid(asymptote=seq(0.4, 1, length.out=n_values),
                     location=seq(0.3, 0.9, length.out=n_values),
                     steepness=seq(0, 6, length.out=n_values))

models$RMSE = 0

for(m in 1:nrow(models)){
  predicted = f(mean_ratings$Image,
                asymptote=models$asymptote[m],
                location=models$location[m],
                steepness=models$steepness[m])
  models$RMSE[m] = rmse(predicted, mean_ratings$Rating)
}

head(models)

print(models[models$RMSE == min(models$RMSE),])

fig_fit = ggplot(models, aes(x=location, y=asymptote, z=RMSE, fill=RMSE)) +
  geom_raster() +
  geom_contour(color='black') +
  scale_fill_gradient(low='red', high='yellow') +
  facet_wrap(~ steepness, labeller=label_both)

print(fig_fit)
```

Let's now try out `nls()` for these data. We use the default algorithm to begin with.

```{r}
starting_values = list(asymptote=0.7, location=0.5, steepness=3)

fit = nls(Rating ~ f(Image, asymptote, location, steepness), mean_ratings,
          starting_values,
          trace=TRUE)

print(fit)
```

## Controlling and checking the fit

In most cases, unless we have a fairly exotic function or data, the choice of starting point won't matter for the final result. The algorithm searches in the direction of better and better fits, so it will eventually find the best combination.

We can check this for our example.

```{r}
starting_values = list(asymptote=0.8, location=0.4, steepness=2)

fit = nls(Rating ~ f(Image, asymptote, location, steepness), mean_ratings,
          starting_values,
          trace=TRUE)

print(fit)
```

But the starting values can sometimes have a big impact on whether the search succeeds at all. For some functions, certain combinations of parameter values may produce invalid or infinite predicted values, or predicted values that are so far off the observed values that it is unclear in which direction the fit might get better.

```{r, error=TRUE}
starting_values = list(asymptote=0.9, location=0.5, steepness=0.1)

nls(Rating ~ f(Image, asymptote, location, steepness), mean_ratings,
    starting_values)
```

If we find that the fitting process is returning errors of this kind, we may be able to help things along by setting boundaries on where the search algorithm may go during its search. `nls()` allows us to input lower and upper bounds for the search. If we want to leave any boundaries open, we can input `Inf` (or `-Inf` if we want to leave a lower boundary open). The default fitting algorithm in `nls()` does not allow for bounds, so we must also switch to another algorithm that does, the 'port' algorithm.

```{r}
starting_values = list(asymptote=0.9, location=0.5, steepness=0.1)

fit = nls(Rating ~ f(Image, asymptote, location, steepness), mean_ratings, starting_values,
          algorithm='port',
          lower=list(asymptote=0, location=0, steepness=0),
          upper=list(asymptote=1, location=1, steepness=Inf),
          trace=TRUE)

print(fit)
```

Now we can successfully fit the model, and get the same estimates for the parameters as we did above, when we set no bounds but used a better guess for the starting values.

We are ready to check how well the estimated model fits the observed mean ratings. The models that `nls()` creates can also be input into the `predict()` function. So we can use `predict()` together with some made-up image values to generate the model's predicted values, which we can then add to our plot as a line.

```{r}
f_data = data.frame(Image=seq(0, 1, length.out=100))
f_data$Rating = predict(fit, f_data)

fig_crabsters = fig_crabsters +
  geom_line(data=f_data, lty='dashed')

print(fig_crabsters)
```

The function seems to approximate the trend in the averaged data reasonably well.

# Models as summaries

In the example we just looked at, with the crab and lobster images, the authors were not especially interested in comparing the chosen model to other models, nor with fitting it to the averaged ratings as we just did. Rather, they wanted to use the model as a way of summarizing different features of a given participant's behavior.

If participants have given lots of responses to lots of different kinds of stimuli, it can be difficult to compare participants using just their raw data. For example, here it would mean comparing mean ratings over several different images. An alternative is to fit a model to each participant's data, and use the estimated parameters of that model as a summary of the participant's behavior. This can be especially useful if we use a model whose parameters each characterize a specific aspect of behavior, as is the case with the model above. In particular the 'steepness' parameter here tells us to what extent a participant sharply changes their categorization of the image at some point along the crab-lobster continuum (as opposed to gradually changing it as the image gradually changes).

As we saw in the individual plots that we created above, the pattern of ratings varies quite a lot from participant to participant in this data set. We will now fit the model separately to each participant's data, and store the estimated parameters.

Let's see first how we would do this for just one participant. We can begin by selecting participant 1, whose ID we store in a variable.

The `subset()` function takes as its input a data frame, and gives us only those rows of the data frame where a certain condition holds. In this case, the condition is that the ID column should be equal to (`==`) 1.

```{r}
p = 1
p_ratings = subset(mean_ratings, ID==p)

p_ratings
```

We can then re-fit the model to just this subset of data. The `update()` function re-fits a model. We have to tell `update()` what aspect of the original fitting procedure we want to change. In this case it is the data.

While we are at it, we can change one other aspect of the fitting procedure in order to make it a little more robust to error. We can change the starting values. Now that we have estimated the best fitting values for the averaged ratings, we can use these as the starting point to search for each participant's best fitting values.

(We can extract fitted parameter values with `coefficients()`.)

```{r}
p_fit = update(fit, data=p_ratings, start=coefficients(fit))

print(p_fit)
```

We can see that this participant has best fitting values for the three parameters that are slightly different from those that best fit the overall average ratings.

Now we want to do this for every participant in turn. We will need somewhere to store the results. For this, we create a new data frame with a column for participant ID (which we get by asking what the unique values of ID are in the original data frame), and columns for the three model parameters, which we initially fill with the value 0. In addition, we create a column for the RMSE of each participant's fitted model. This will allow us to identify any participants for whom the model fits worse than for others.

```{r}
p_models = data.frame(ID=unique(mean_ratings$ID),
                      asymptote=0,
                      location=0,
                      steepness=0,
                      RMSE=0)
```

Now we do as above, but with a loop in which the value of `p` changes on each iteration. Storing the estimated parameters is slightly tricky. For this, we need to pick out the right row of our new data frame, and the right columns, and replace them with the estimated values from the participant's fitted model.

(We also turn off the `trace` option, so we aren't flooded with output on all the attempted parameters values for every participant.)

```{r}
for(p in unique(p_models$ID)){
  
  p_ratings = subset(mean_ratings, ID==p)
  p_fit = update(fit, data=p_ratings, start=coefficients(fit), trace=FALSE)
  
  p_row = p_models$ID==p
  
  p_models[p_row,c('asymptote','location','steepness')] = coefficients(p_fit)
  p_models$RMSE[p_row] = rmse(predict(p_fit), p_ratings$Rating)
  
}

p_models
```

What spread of values did we get for the fitted parameters? We can look at histograms of each one.

To avoid repeating the plotting commands three times, we can create one plot, and then add to this the `aes()` function to tell the plot what variable to show along the *x* axis.

```{r}
parameter_hist = ggplot(p_models) +
  geom_histogram(bins=20) +
  labs(caption='Data: Reindl et al. (2018)')

print(parameter_hist + aes(x=asymptote))
print(parameter_hist + aes(x=location))
print(parameter_hist + aes(x=steepness))
```

We might also sometimes be interested in whether the fitted values are related to one another. This is not particularly of interest in this example, but we will check anyway for the sake of completeness.

The 'GGally' package gives us some additional plotting functions for specific applications, such as `ggpairs()`, which plots a matrix of scatterplots, each of which plots the relationship between two of a set of variables within a data set. We tell `ggpairs()` which columns from the data to include in the plot.

We see that the location and steepness parameters are positively related to each other, for example.

```{r}
ggpairs(p_models, columns=c('asymptote','location','steepness'))
```

We have now obtained and explored fitted parameter values for each participant. These summarize a participant's performance in a way that the mean ratings per image could not easily capture.

Because the fitting process can be computationally somewhat time-consuming for larger data sets or complex models, it is often a good idea to save the data frame of fitted parameter values to a new file, in case we want to pick up analysis of it later.

Saving to csv format can be done with `write.csv()`.

```{r}
write.csv(p_models, 'crabsters_parameters.csv')
```

With this accomplished, we can go on to compare the fitted values across different experimental conditions, or across types of participants, as was one of the objectives of the original study. This would put us back in familiar statistical territory, comparing values across groups and so on. For this reason, we won't go on to do it here. Instead, we will look at one more important aspect of model fitting.

## Checking model fit

Because we applied boundaries to the search for best fitting values, we should first check that no participants actually got fitted values on one of the boundaries. This would indicate either that the boundaries were not appropriate and have artificially constrained the possible estimates we can get, or that something just went wrong with the fitting algorithm.

(The `range()` function gives us the minimum and maximum.)

Here we see that there are no fitted values on any of the boundaries.

```{r}
print(range(p_models$asymptote))
print(range(p_models$location))
print(range(p_models$steepness))
```

We should also check whether the model did not drastically fail to fit any of the participants' data. For this, we can look first at the general spread of RMSE values using the same histogram from above.

```{r}
print(parameter_hist + aes(x=RMSE))
```

The worst RMSE values are at around 0.08. Given that the values of RMSE are on the scale of the ratings, which range from 0 to 1, this is not a really huge error.

We can locate the worst fits by ranking the RMSE values as bars, and displaying the participant ID numbers on the bars.

This requires some more plotting tricks:

* the `reorder()` function sorts the display order for one variable, based on the values of a second varable (here we sort the ID numbers by RMSE)
* the `label` plotting dimension in `aes()` assigns the specific values of a variable as labels on the plot
* `geom_text()` then applies the `label` from the main plot definition as plain text
* we remove the labelling of the *x* axis entirely, since we already have the IDs as labels

```{r, fig.width=14}
rmse_ranks = ggplot(p_models, aes(y=RMSE, x=reorder(ID,RMSE), label=ID)) +
  geom_col() +
  geom_text(vjust=0) +
  scale_x_discrete(breaks=NULL, name=NULL)

print(rmse_ranks)
```

We can pick out the ID numbers of the worst fits by applying a subset condition.

```{r}
bad_fits = p_models$ID[p_models$RMSE>0.07]

print(bad_fits)
```

If you look back at our individual participant plots, you will see that unlike most of the others, these participants have a combination of two features that the model does not account for.

First, they have a steady and fairly steep increase in ratings as the image changes. So they don't seem to classify the approximately crab-like images together with the more clearly crab-like ones. And nor do their ratings reach a maximum of lobsterness.

Additionally, these participants have a small 'plateau' in their ratings in the middle of the image morphing scale, as if they were perhaps classifying the most ambiguous images as belonging to a third, intermediate category, 'crabster'. Such a break in the transition from crab to lobster is not a feature of the model.

```{r, fig.width=12, fig.height=9}
fig_crabsters_p = fig_crabsters_p +
  aes(color=ID%in%bad_fits) +
  scale_color_manual(values=c('black','red')) +
  theme(legend.position='none')

print(fig_crabsters_p)
```
